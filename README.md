# Understanding

*This is just me trying to understand some concepts. Feel free to ignore. Or not.*

## Run things

```shell
# Run demo
python3 demo.py

# Test
python3 -m unittest

# Type-check
uv run mypy .

# Notebooks
uv jupyter lab
```

## Learning trace

What I have looked at so far:

1. Backpropagation
2. Artificial neuron
    - Inputs, weights, and bias
    - Activation functions
3. Neuronal network
    - Layers
    - Multi-layer perceptron
4. Training
    - Loss function
    - Backpropagation
    - Parameter tuning (gradient descent)

Things I want to learn more about:

- Transformers
- Attention
- Multi-modal models
- Vision language models

## Resources

- [Andrej Karpathy: Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- [Andrej Karpathy: Zero to Hero](https://karpathy.ai/zero-to-hero.html), [YouTube playlist](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
- [Standford CS230: Deep Learning (YouTube playlist)](https://www.youtube.com/playlist?list=PLoROMvodv4rNRRGdS0rBbXOUGA0wjdh1X)

## Tools

- [LLM Visualization](https://bbycroft.net/llm), [GitHub](https://github.com/bbycroft/llm-viz)
- [Tiktokenizer](https://tiktokenizer.vercel.app/), [GitHub](https://github.com/dqbd/tiktokenizer): Visualize how different models split inputs into tokens
